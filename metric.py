import os
import sys
import json
import csv
import requests
import pickle

from scipy import spatial
from tqdm import tqdm
from config import *

# one time call
def create_query_pair_set():
    train_file_path = TRAIN_SESSION_FILE_PATH
    query_pair_set = {}
    count_map = {}
    with open(train_file_path, 'r') as train_file:
        lines = train_file.readlines()
        for line in tqdm(lines):
            text = line.strip().split('\t')
            queries = text[1:]
            i=len(queries)-1
            while i > 0:
                try:
                    query_pair_set[queries[i]].add(queries[i-1])
                except:
                    query_pair_set[queries[i]] = set()
                    query_pair_set[queries[i]].add(queries[i-1])
                try:
                    count_map[(queries[i], queries[i-1])] += 1
                except:
                    count_map[(queries[i], queries[i-1])] = 1
                i = i-1
    pickle.dump(query_pair_set, open(QUERY_PAIR_SET_DATA, "wb"))
    pickle.dump(count_map, open(COUNT_MAP_DATA, "wb"))

def query_pair_set_sanity_check():
    query_pair_set_file_path = QUERY_PAIR_SET_DATA
    query_pair_set_file = open(query_pair_set_file_path, 'rb')
    query_pair_set = pickle.load(query_pair_set_file)
    print("First query pair: " + str(list(query_pair_set.keys())[0]) + " : " + str(query_pair_set[list(query_pair_set.keys())[0]]))
    print("Number of keys in query pair set: " + str(len(query_pair_set.keys())))
    query_pair_set_file.close()

def counts_sanity_check():
    count_map_file_path = COUNT_MAP_DATA
    count_map_file = open(count_map_file_path, 'rb')
    count_map = pickle.load(count_map_file)
    print("First count map: " + str(list(count_map.keys())[0]) + " : " + str(count_map[list(count_map.keys())[0]]))
    print("Number of keys in count map: " + str(len(count_map.keys())))
    count_map_file.close()

def parse_dev_set():
    query_pair_file_path = QUERY_PAIR_SET_DATA
    query_pair_file = open(query_pair_file_path, 'rb')
    query_pair_set = pickle.load(query_pair_file)
    dev_file_path = DEV_SESSION_FILE_PATH
    embeddings = get_embeddings()
    data = []
    with open(dev_file_path, 'r') as dev_file:
        lines = dev_file.readlines()
        for line in tqdm(lines):
            text = line.strip().split('\t')
            queries = text[1:]
            context = queries[:-1]
            try:
                negative_candidates = list(query_pair_set[queries[-1]])
            except:
                continue
            negatives = {}
            for candidate in negative_candidates:
                try:
                    similarity = 1 - spatial.distance.cosine(embeddings[queries[-2]], embeddings[candidate])
                except:
                    similarity = 0
                # threshold can be something between 0.9 and 0.95
                if similarity > QUERY_SIMILARITY_THRESHOLD or similarity == 0:
                    continue
                negatives[candidate] = similarity
            information = {}
            try:
                pos_similarity = similarity = 1 - spatial.distance.cosine(embeddings[queries[-1]], embeddings[queries[-2]])
            except:
                pos_similarity = 0
            information['query'] = queries[-1]
            information['last_query'] = queries[-2]
            information['last_context_sim'] = pos_similarity
            information['context'] = context
            information['negatives'] = negatives
            data.append(information)
    pickle.dump(data, open(DEV_PARSE_DATA, "wb"))

def dev_set_parse_sanity_check():
    dev_parse_file_path = DEV_PARSE_DATA
    dev_parse_file = open(dev_parse_file_path, 'rb')
    dev_parse = pickle.load(dev_parse_file)
    print("First object of dev file in its entirety: " + str(dev_parse[0]))
    print("Number of objects obtained by parsing dev set: " + str(len(dev_parse)))

def get_embeddings():
    embeddings_path = EMBEDDINGS_DATA
    embeddings_file = open(embeddings_path, 'rb')
    embeddings = pickle.load(embeddings_file)
    return embeddings

# csv file generated by get_vectors file (taken from Ayush Dalmia and group)
# one time call
def save_embeddings_from_csv():
    embed_file_path = DATA_DIR + '/embeddings.csv'
    embeddings = {}
    with open(embed_file_path, 'r') as embed_file:
        lines = embed_file.readlines()
        for line in lines:
            # ,"[ is the splitting pattern between query and vector
            line = line.strip()
            idx_open_par = line.find(',"[')
            query = line[:idx_open_par]
            vector_str = line[idx_open_par+3:len(line)-2]
            try:
                vector = [float(v) for v in vector_str.split(',')]
                embeddings[query] = vector
            except:
                print(line)
    pickle.dump(embeddings, open(EMBEDDINGS_DATA, "wb"))

def sim_mean_reciprocal_rank():
    print("Calculating simmrr metric")
    dev_parse_file_path = DEV_PARSE_DATA
    dev_parse_file = open(dev_parse_file_path, 'rb')
    dev_parse = pickle.load(dev_parse_file)
    sum_mrr = 0
    for information in tqdm(dev_parse):
        # how similar is final query to next to last query
        last_context_similarity = information['last_context_sim']
        negatives = information['negatives']
        sim_scores = [last_context_similarity]
        for k,v in negatives.items():
            sim_scores.append(v)
        sorted_sim_scores = sorted(sim_scores, reverse=True)
        query_rank = sorted_sim_scores.index(last_context_similarity)
        if query_rank == 0:
            query_mrr = 1
        else:
            query_mrr = 1/query_rank
        sum_mrr += query_mrr
    mean_mrr = sum_mrr/len(dev_parse)
    dev_parse_file.close()
    return mean_mrr

def baseline():
    print("Calculating simple MRR baseline")
    dev_parse_file_path = DEV_PARSE_DATA
    dev_parse_file = open(dev_parse_file_path, 'rb')
    dev_parse = pickle.load(dev_parse_file)
    count_map_file_path = COUNT_MAP_DATA
    count_map_file = open(count_map_file_path, 'rb')
    count_map = pickle.load(count_map_file)
    sum_mrr = 0
    for information in tqdm(dev_parse):
        query_mrr = mean_reciprocal_rank(information['last_query'], information['query'], count_map)
        sum_mrr += query_mrr
    count_map_file.close()
    return sum_mrr

# can be sped up a lot by using better data structures to store count map
def mean_reciprocal_rank(query, next_query, count_map):
    # MRR for the dev dump
    all_counts = []
    for counter in count_map.keys():
        if counter[1] == query and counter[0] != next_query:
            all_counts.append(count_map[counter])
    try:
        all_counts.append(count_map[(next_query, query)])
    except:
        try:
            return (1/len(all_counts))
        except:
            return 0
    sorted_counts = sorted(all_counts, reverse=True)
    rank = all_counts.index(count_map[(next_query, query)])
    if rank != 0
        reciprocal_rank = 1/rank
    else:
        reciprocal_rank = 1
    return reciprocal_rank

if __name__=='__main__':
    save_embeddings_from_csv()
    create_query_pair_set()
    parse_dev_set()
    print(sim_mean_reciprocal_rank())
    print(baseline())